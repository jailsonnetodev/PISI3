{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "hLokmrlOZ66S"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jailsonnetodev/PISI3/blob/main/Grupo_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explorando o dataset US Used cars dataset"
      ],
      "metadata": {
        "id": "MA28Fi8CZY_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "4x6UoJVAl4Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/usedcars_usa.csv')\n",
        "df.shape"
      ],
      "metadata": {
        "id": "dYkXm3tjmgjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "o5m5f9BsmjGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dicionario de dados"
      ],
      "metadata": {
        "id": "osgSs26ImQTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    'vin': 'Número de identificação do veículo - manter',\n",
        "    'back_legroom': 'Espaço para as pernas no banco traseiro - usar a média',\n",
        "    'bed': 'Caçamba - descartar (muitos valores nulos)',\n",
        "    'bed_height': 'Altura da caçamba - descartar (muitos valores nulos)',\n",
        "    'bed_length': 'Comprimento da caçamba - descartar (muitos valores nulos)',\n",
        "    'body_type': 'Tipo de carroceria - usar a moda',\n",
        "    'cabin': 'Cabine - descartar (muitos valores nulos)',\n",
        "    'city': 'Cidade - manter',\n",
        "    'city_fuel_economy': 'Consumo de combustível na cidade - usar a média',\n",
        "    'combine_fuel_economy': 'Consumo combinado de combustível - descartar',\n",
        "    'daysonmarket': 'Dias no mercado - manter',\n",
        "    'dealer_zip': 'CEP do revendedor - manter',\n",
        "    'description': 'Descrição - manter',\n",
        "    'engine_cylinders': 'Cilindros do motor - usar a moda',\n",
        "    'engine_displacement': 'Cilindrada do motor - usar a média',\n",
        "    'engine_type': 'Tipo de motor - usar a moda',\n",
        "    'exterior_color': 'Cor exterior - usar a moda',\n",
        "    'fleet': 'Frota - usar a moda',\n",
        "    'frame_damaged': 'Chassi danificado - usar a moda',\n",
        "    'franchise_dealer': 'Concessionária franqueada - manter',\n",
        "    'franchise_make': 'Marca da franquia - usar a moda e tratar valores None',\n",
        "    'front_legroom': 'Espaço para as pernas no banco dianteiro - usar a média',\n",
        "    'fuel_tank_volume': 'Volume do tanque de combustível - usar a média',\n",
        "    'fuel_type': 'Tipo de combustível - usar a moda',\n",
        "    'has_accidents': 'Histórico de acidentes - usar a moda e converter para booleano',\n",
        "    'height': 'Altura - usar a média',\n",
        "    'highway_fuel_economy': 'Consumo de combustível na estrada - usar a média',\n",
        "    'horsepower': 'Potência - usar a média',\n",
        "    'interior_color': 'Cor interior - usar a moda',\n",
        "    'isCab': 'É caminhonete (cabine) - usar a moda e converter para booleano',\n",
        "    'is_certified': 'É certificado - manter',\n",
        "    'is_cpo': 'Certificado de proprietário anterior (CPO) - manter',\n",
        "    'is_new': 'É novo - manter',\n",
        "    'is_oemcpo': 'Certificado pelo fabricante (OEM CPO) - manter',\n",
        "    'latitude': 'Latitude - manter',\n",
        "    'length': 'Comprimento - usar a média',\n",
        "    'listed_date': 'Data de listagem - manter',\n",
        "    'listing_color': 'Cor da listagem - manter',\n",
        "    'listing_id': 'ID da listagem - manter',\n",
        "    'longitude': 'Longitude - manter',\n",
        "    'main_picture_url': 'URL da imagem principal - manter',\n",
        "    'major_options': 'Opções principais - manter',\n",
        "    'make_name': 'Nome da marca - manter',\n",
        "    'maximum_seating': 'Capacidade máxima de assentos - converter para inteiro e usar a média',\n",
        "    'mileage': 'Quilometragem - usar a média',\n",
        "    'model_name': 'Nome do modelo - manter',\n",
        "    'owner_count': 'Número de proprietários - usar a média',\n",
        "    'power': 'Potência - usar a moda',\n",
        "    'price': 'Preço - manter',\n",
        "    'salvage': 'Recuperado - usar a moda e converter para booleano',\n",
        "    'savings_amount': 'Valor economizado - manter',\n",
        "    'seller_rating': 'Avaliação do vendedor - usar a média',\n",
        "    'sp_id': 'ID do vendedor - manter',\n",
        "    'sp_name': 'Nome do vendedor - manter',\n",
        "    'theft_title': 'Título de roubo - usar a moda e converter para booleano',\n",
        "    'torque': 'Torque - usar a moda',\n",
        "    'transmission': 'Transmissão - usar a moda',\n",
        "    'transmission_display': 'Exibição da transmissão - usar a moda',\n",
        "    'trimId': 'ID da versão - manter',\n",
        "    'trim_name': 'Nome da versão - usar a moda',\n",
        "    'vehicle_damage_category': 'Categoria de danos do veículo - descartar (todos os valores nulos)',\n",
        "    'wheel_system': 'Sistema de rodas - usar a moda',\n",
        "    'wheel_system_display': 'Exibição do sistema de rodas - usar a moda',\n",
        "    'wheelbase': 'Entre-eixos - usar a moda e converter para float',\n",
        "    'width': 'Largura - usar a média',\n",
        "    'year': 'Ano - manter'\n",
        "}"
      ],
      "metadata": {
        "id": "Gxi5VSZHqWDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessamento e transformacao dos dados .parquet"
      ],
      "metadata": {
        "id": "Wit6QielZjpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_amostra(file_path,amostra, num_amostras=100000):\n",
        "    if os.path.exists(amostra):\n",
        "        print(f\"O arquivo Parquet '{amostra}' já existe.\")\n",
        "    else:\n",
        "        print(f\"Gerando amostra de {num_amostras} registros e salvando como {amostra}...\")\n",
        "        df = pd.read_csv(file_path, low_memory=False, skiprows=lambda i: i > 0 and i % (3000000 // num_amostras) != 0)\n",
        "        df.to_csv(amostra,index=False)\n",
        "        print(f\"Amostra gerada e salva como '{amostra}'.\")\n",
        "\n",
        "\n",
        "\n",
        "file_path = 'usedcars_usa.csv'\n",
        "amostra = 'usedcars_usa100k.csv'\n",
        "gerar_amostra(file_path, amostra, num_amostras=100000)\n",
        "df = pd.read_csv('usedcars_usa100k.csv')\n",
        "\n",
        "\n",
        "\n",
        "drop_columns = ['vin','bed','bed_height','bed_length','cabin','combine_fuel_economy','dealer_zip','description','is_certified','is_cpo','is_oemcpo','latitude','listing_id','longitude','main_picture_url','sp_id','trimId','vehicle_damage_category','major_options']\n",
        "\n",
        "#Dropar as colunas especificas\n",
        "def drop_columns_from_df(df, columns_to_drop):\n",
        "  df = df.drop(columns=columns_to_drop)\n",
        "  return df\n",
        "df = drop_columns_from_df(df, drop_columns)\n",
        "\n",
        "\n",
        "#tratar outlier com o metodo IQR\n",
        "def drop_outliers(df, columns, k=1.5):\n",
        "    for column in columns:\n",
        "        q1 = df[column].quantile(0.25)\n",
        "        q3 = df[column].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        df[column] = df[column].clip(lower=q1 - k * iqr, upper=q3 + k * iqr)\n",
        "    return df\n",
        "\n",
        "# prompt: funcao para extrair o numero da coluna maximum_seating e converter para inteiro , atribuir zero em caso de valores nulos , depois tratar os valores nulos e zero com a media\n",
        "\n",
        "def extract_number_to_int(df, column):\n",
        "  df[column] = df[column].str.extract('(\\d+)').astype(float).fillna(0).astype(int)\n",
        "  mean_value = df[column].mean()\n",
        "  df[column] = df[column].replace(0, mean_value)\n",
        "  df[column] = df[column].astype(int)\n",
        "  return df\n",
        "df = extract_number_to_int(df, 'maximum_seating')\n",
        "\n",
        "\n",
        "extract_number = ['back_legroom','height','front_legroom','fuel_tank_volume','length','width','wheelbase']\n",
        "\n",
        "# prompt: funcao para extrair apenas o numero de cada coluna acima da lista e converter pra float\n",
        "\n",
        "def extract_number_to_float(df, columns, decimal_places=1):\n",
        "    for column in columns:\n",
        "        df[column] = (\n",
        "            df[column]\n",
        "            .str.extract('(\\d+\\.\\d+|\\d+)')\n",
        "            .astype(float)\n",
        "            .round(decimal_places)\n",
        "        )\n",
        "    drop_outliers(df, columns, k=1.5)\n",
        "    return df\n",
        "df = extract_number_to_float(df, extract_number, decimal_places=1)\n",
        "\n",
        "\n",
        "col_categories_process = ['has_accidents','body_type','engine_cylinders','engine_type','exterior_color','fleet','frame_damaged','franchise_make','fuel_type','has_accidents','interior_color','isCab','theft_title','torque','transmission','transmission_display','trim_name','wheel_system','wheel_system_display','wheelbase','power','salvage']\n",
        "\n",
        "# prompt: funcao para usar a moda passando as colunas acima\n",
        "\n",
        "def impute_mode(df, columns):\n",
        "  for column in columns:\n",
        "    mode_value = df[column].mode()[0]\n",
        "    df[column] = df[column].fillna(mode_value)\n",
        "  return df\n",
        "\n",
        "df = impute_mode(df, col_categories_process)\n",
        "\n",
        "\n",
        "\n",
        "converter_int = ['owner_count','horsepower','city_fuel_economy','engine_displacement','highway_fuel_economy','seller_rating']\n",
        "\n",
        "\n",
        "def convert_to_int(df, colunas):\n",
        "    for coluna in colunas:\n",
        "        # Substituir valores nulos pela média da coluna\n",
        "        media = df[coluna].mean()\n",
        "        df[coluna].fillna(media, inplace=True)\n",
        "\n",
        "        # Converter a coluna para inteiro\n",
        "        df[coluna] = df[coluna].astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = convert_to_int(df, converter_int)\n",
        "\n",
        "\n",
        "col_media_process = ['back_legroom','front_legroom','fuel_tank_volume','height','length','mileage','seller_rating','width']\n",
        "\n",
        "\n",
        "# prompt: funcao para preencher com o valor da media as colunas acima\n",
        "\n",
        "def impute_mean(df, columns):\n",
        "  for column in columns:\n",
        "    mean_value = df[column].mean()\n",
        "    df[column] = df[column].fillna(mean_value)\n",
        "  return df\n",
        "\n",
        "df = impute_mean(df, col_media_process)\n",
        "\n",
        "\n",
        "\n",
        "#TRADUZINDO TODAS AS COLUNAS EM PT-BR PARA MELHOR ENTENDIMENTO NO PROCESSO DE ANALISE\n",
        "\n",
        "\n",
        "# Dicionário de tradução das colunas\n",
        "traducoes = {\n",
        "    'back_legroom': 'espaco_banco_traseiro',\n",
        "    'body_type': 'tipo_carroceria',\n",
        "    'city': 'cidade',\n",
        "    'city_fuel_economy': 'consumo_cidade',\n",
        "    'daysonmarket': 'dias_no_mercado',\n",
        "    'engine_cylinders': 'cilindros_motor',\n",
        "    'engine_displacement': 'cilindradas_motor',\n",
        "    'engine_type': 'tipo_motor',\n",
        "    'exterior_color': 'cor_exterior',\n",
        "    'fleet': 'frota',\n",
        "    'frame_damaged': 'chassi_danificado',\n",
        "    'franchise_dealer': 'concessionaria_franqueada',\n",
        "    'franchise_make': 'marca_da_franquia',\n",
        "    'front_legroom': 'espaco_banco_dianteiro',\n",
        "    'fuel_tank_volume': 'volume_tanque',\n",
        "    'fuel_type': 'tipo_combustivel',\n",
        "    'has_accidents': 'historico_acidente',\n",
        "    'height': 'altura',\n",
        "    'highway_fuel_economy': 'consumo_estrada',\n",
        "    'horsepower': 'cavalo_de_potencia',\n",
        "    'interior_color': 'cor_interior',\n",
        "    'isCab': 'ee_cabine',\n",
        "    'is_new': 'ee_novo',\n",
        "    'length': 'comprimento',\n",
        "    'listed_date': 'data_listagem',\n",
        "    'listing_color': 'cor_listagem',\n",
        "    'make_name': 'nome_marca',\n",
        "    'maximum_seating': 'maximo_assentos',\n",
        "    'mileage': 'quilometragem',\n",
        "    'model_name': 'nome_modelo',\n",
        "    'owner_count': 'qtd_proprietarios',\n",
        "    'power': 'potencia',\n",
        "    'price': 'preco',\n",
        "    'salvage': 'recuperado',\n",
        "    'savings_amount': 'valor_economizado',\n",
        "    'seller_rating': 'avaliacao_vendedor',\n",
        "    'sp_name': 'nome_vendedor',\n",
        "    'theft_title': 'titulo_roubo',\n",
        "    'torque': 'torque',\n",
        "    'transmission': 'transmissao',\n",
        "    'transmission_display': 'exibicao_transmissao',\n",
        "    'trim_name': 'nome_versao',\n",
        "    'wheel_system': 'sistema_rodas',\n",
        "    'wheel_system_display': 'exibicao_sistema_rodas',\n",
        "    'wheelbase': 'entre_eixos',\n",
        "    'width': 'largura',\n",
        "    'year': 'ano',\n",
        "}\n",
        "\n",
        "# Renomear as colunas usando o dicionário de traduções\n",
        "df = df.rename(columns=traducoes)\n",
        "\n",
        "def categorizar_daysonmarket(df):\n",
        "    bins = [0, 36, 83, 185, 365, float('inf')]\n",
        "    labels = ['ate-36 dias', '36-83 dias', '83-185 dias', '185-365 dias', '> 365 dias']\n",
        "    df['dias_no_mercado_label'] = pd.cut(df['dias_no_mercado'], bins=bins, labels=labels, right=False)\n",
        "    return df\n",
        "\n",
        "df = categorizar_daysonmarket(df)\n",
        "\n",
        "\n",
        "\n",
        "def tratar_booleans(df):\n",
        "    col_bool = df.select_dtypes(include=[bool]).columns\n",
        "    df[col_bool] = df[col_bool].astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = tratar_booleans(df)\n",
        "\n",
        "\n",
        "#OBTENDO TODAS AS COLUNAS NUMERICAS\n",
        "\n",
        "\n",
        "def get_numeric_columns(df):\n",
        "    # Seleciona colunas que são do tipo int64 ou float64\n",
        "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    return numeric_columns\n",
        "\n",
        "col_numerics = get_numeric_columns(df)\n",
        "\n",
        "\n",
        "#DEFININDO COLUNAS QUE MESMO TENDO VALORES DISCREPANTES SERAO MANTIDOS POIS SE TRATAM DE OUTLIERS NATURAIS E SERA REPROCESSADO NO MODELO COM E SEM\n",
        "exclude_outliers = ['maximo_assentos','dias_no_mercado','qtd_proprietarios','avaliacao_vendedor','frota','chassi_danificado','concessionaria_franqueada','historico_acidente','ee_cabine','ee_novo','recuperado','titulo_roubo']\n",
        "\n",
        "\n",
        "#REDEFININDO AS COLUNAS QUE SERAO APLICADAS A FUNÇÃO DE TRTAMENTO DE OUTLIERS\n",
        "new_col_numerics = [ col for col in col_numerics if col not in exclude_outliers]\n",
        "\n",
        "\n",
        "\n",
        "#APLICANDO OUTLIERS\n",
        "\n",
        "def drop_outliers(df, columns, k=1.5,):\n",
        "    for column in columns:\n",
        "        q1 = df[column].quantile(0.25)\n",
        "        q3 = df[column].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        df[column] = df[column].clip(lower=q1 - k * iqr, upper=q3 + k * iqr)\n",
        "    return df\n",
        "df = drop_outliers(df,new_col_numerics,k=1.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#transformando em .parquet e gerando amostras menores\n",
        "new_data = df.to_parquet('usedcars_usa.parquet')\n",
        "\n",
        "\n",
        "def random_parquet(path: str, num: int) ->None:\n",
        "  data = pd.read_parquet(path)\n",
        "  new_data = data.sample(num, replace=False)\n",
        "  num2 = ''.join(reversed(''.join(reversed(f'{num}')).replace('000','k')))\n",
        "  new_data.to_parquet(path.replace('.',f'{num2}.'))\n",
        "\n",
        "\n",
        "for i in [20000,50000,100000]:\n",
        "  random_parquet('usedcars_usa.parquet',i)\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_parquet('usedcars_usa100k.parquet')"
      ],
      "metadata": {
        "id": "izVcp-NDqht3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analise Exploratoria"
      ],
      "metadata": {
        "id": "hLokmrlOZ66S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_parquet('data/usedcars_usa.parquet')\n",
        "\n",
        "\n",
        "col1,col2 = st.columns([0.3,0.8])\n",
        "\n",
        "with col1:\n",
        "    top_categorics = st.slider('Selecione quantidade de marcas que deseja visualizar:', 0, 20, 5)\n",
        "with col2:\n",
        "    data_filtered = top_categories(\n",
        "    data = data,\n",
        "    top=top_categorics,\n",
        "    label='nome_marca'\n",
        "    )\n",
        "    new_fig = px.bar(data_filtered, x='nome_marca', y='preco')\n",
        "    new_fig\n",
        "\n",
        "\n",
        "group_make =  data.groupby(['nome_marca'])['preco'].mean().reset_index().sort_values(by='preco',ascending=False)\n",
        "\n",
        "\n",
        "\n",
        "fig =plt.figure(figsize = (25 , 10))\n",
        "sns.heatmap(data.select_dtypes('float' , 'int').corr() , annot = True)\n",
        "plt.xticks(rotation = 45);\n",
        "st.pyplot(fig)\n",
        "\n",
        "\n",
        "df_ano = data.groupby('ano')['dias_no_mercado'].mean().reset_index()\n",
        "fig = px.line(df_ano, x='ano', y='dias_no_mercado',\n",
        "              title='Relação entre ano de fabricação e tempo de permanência')\n",
        "st.plotly_chart(fig)\n",
        "\n",
        "\n",
        "df_acidentes = data.groupby('historico_acidente')['dias_no_mercado'].mean().reset_index()\n",
        "fig2 = px.bar(df_acidentes, x='historico_acidente', y='dias_no_mercado',\n",
        "             title='Impacto de histórico de acidentes no tempo de venda')\n",
        "st.plotly_chart(fig2)\n",
        "\n",
        "\n",
        "\n",
        "df_cor = data.groupby('cor_exterior')['dias_no_mercado'].mean().reset_index()\n",
        "\n",
        "datafiltered = top_categories(\n",
        "    data=df_cor,\n",
        "    top=10,\n",
        "    label='cor_exterior'\n",
        ")\n",
        "fig3 = px.bar(datafiltered, x='cor_exterior', y='dias_no_mercado',\n",
        "             title='Relação entre cor do veículo e tempo de venda')\n",
        "st.plotly_chart(fig3)\n",
        "\n",
        "\n",
        "\n",
        "df_combustivel = data.groupby('tipo_combustivel')['preco'].mean().reset_index()\n",
        "fig4 = px.bar(df_combustivel, x='tipo_combustivel', y='preco', title='Preferência por tipo de combustível')\n",
        "title=('Relação entre cor do veículo e tempo de venda')\n",
        "st.plotly_chart(fig4)\n",
        "\n",
        "\n",
        "df_veiculos_ano = data.groupby(['ano','dias_no_mercado_label']).size().reset_index(name='total_veiculos')\n",
        "fig5 = px.bar(df_veiculos_ano, x='ano', y='total_veiculos', title='Total de veículos por ano', color='dias_no_mercado_label')\n",
        "st.plotly_chart(fig5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_preco_ano = data.groupby('ano')['preco'].mean().reset_index()\n",
        "fig6 = px.line(df_preco_ano, x='ano', y='preco', title='Preço médio por ano')\n",
        "st.plotly_chart(fig6)\n",
        "\n",
        "\n",
        "\n",
        "seats_df = data.groupby(['maximo_assentos' , 'tipo_carroceria'])['tipo_carroceria'].count().to_frame().rename(columns = {'tipo_carroceria':'Count'}).reset_index()\n",
        "fig7 = px.bar(seats_df, x=\"tipo_carroceria\", y=\"Count\", animation_frame=\"maximo_assentos\", animation_group=\"maximo_assentos\",\n",
        "            color=\"Count\")\n",
        "fig7[\"layout\"].pop(\"updatemenus\") # optional, drop animation buttons\n",
        "st.plotly_chart(fig7)\n",
        "\n",
        "\n",
        "fig8 = px.box(data,x='quilometragem',y='dias_no_mercado_label')\n",
        "fig8\n",
        "\n",
        "\n",
        "\n",
        "group_by_year = data.groupby(['ano','dias_no_mercado_label']).agg({'preco': np.mean}).reset_index()\n",
        "fig9 = px.line(group_by_year, x='ano', y='preco', color='dias_no_mercado_label')\n",
        "fig9\n",
        "\n",
        "\n",
        "group_km =  data.groupby(['dias_no_mercado'])['quilometragem'].size().reset_index()\n",
        "fig10 = px.line(group_km, x='dias_no_mercado',y='quilometragem')\n",
        "fig10\n",
        "\n",
        "\n",
        "group_make =  data.groupby(['nome_marca'])['preco'].size().reset_index().sort_values(by='preco',ascending=False)\n",
        "\n",
        "fig11 = px.bar(group_make, x='nome_marca', y='preco')\n",
        "fig11\n",
        "\n",
        "\n",
        "\n",
        "group_ano_km_preco = data.groupby(['ano','quilometragem'])['preco'].mean().reset_index()\n",
        "fig12 = px.bar(group_ano_km_preco, x=\"ano\", y=[\"quilometragem\", \"preco\"], barmode='group',\n",
        "            title=\"Total de Veículos e Preço Médio por Ano\")\n",
        "st.plotly_chart(fig12)\n",
        "\n",
        "data_make = top_categories(\n",
        "    data=data,\n",
        "    top=5,\n",
        "    label='nome_marca'\n",
        ")\n",
        "\n",
        "\n",
        "fig = px.bar(data_make, x='nome_marca', y='dias_no_mercado', color='ano',\n",
        "             title='Tempo de permanência no mercado por modelo e ano')\n",
        "fig"
      ],
      "metadata": {
        "id": "8cm8rRyiqfcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning"
      ],
      "metadata": {
        "id": "BftkdpmcndSW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZB-rZ0O3qge-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering"
      ],
      "metadata": {
        "id": "fgAanhIVnqCd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GB1_KUijqi7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classificacao"
      ],
      "metadata": {
        "id": "EtXWEFzHnw-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import streamlit  as st\n",
        "import os\n",
        "import pickle\n",
        "from utils.transform_pkl import main\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "\n",
        "\n",
        "def table_report(y_test: np.ndarray, previsao: np.ndarray, method:str =''):\n",
        "  #st.markdown(f'##### Classification report do metodo:  <span style=\"color: blue\">{method}</span>', unsafe_allow_html=True)\n",
        "  report = classification_report(y_test, previsao, output_dict=True)\n",
        "  classification_data = pd.DataFrame(report).transpose()\n",
        "  print(classification_data)\n",
        "\n",
        "def table_report(y_test: np.ndarray, previsao: np.ndarray, method:str =''):\n",
        "  #st.markdown(f'##### Classification report do metodo:  <span style=\"color: blue\">{method}</span>', unsafe_allow_html=True)\n",
        "  report = classification_report(y_test, previsao, output_dict=True)\n",
        "  classification_data = pd.DataFrame(report).transpose()\n",
        "  #st.table(classification_data)\n",
        "  print(classification_data)\n",
        "def random_forest(\n",
        "  x_training: np.ndarray, y_training: np.ndarray, x_test: np.ndarray, y_test: np.ndarray\n",
        "  )-> None:\n",
        "  st.markdown('### Resultado do machine learning usando o método Random Forest')\n",
        "\n",
        "  if not(os.path.isfile('random_forest.pkl')):\n",
        "    obj_random_forest = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\n",
        "    obj_random_forest.fit(x_training, y_training)\n",
        "    with open('random_forest.pkl', mode='wb') as f:\n",
        "      pickle.dump(obj_random_forest, f)\n",
        "  else:\n",
        "    with open('random_forest.pkl', 'rb') as f:\n",
        "      obj_random_forest = pickle.load(f)\n",
        "\n",
        "  prevision_random_forest = obj_random_forest.predict(x_test)\n",
        "  importances = pd.Series(\n",
        "    data=obj_random_forest.feature_importances_,\n",
        "    index= ['espaco_banco_traseiro', 'tipo_carroceria', 'cidade', 'consumo_cidade',\n",
        "       'cilindros_motor', 'cilindradas_motor', 'tipo_motor',\n",
        "       'cor_exterior', 'frota', 'chassi_danificado',\n",
        "       'concessionaria_franqueada', 'marca_da_franquia',\n",
        "       'espaco_banco_dianteiro', 'volume_tanque', 'tipo_combustivel',\n",
        "       'historico_acidente', 'altura', 'consumo_estrada', 'cavalo_de_potencia',\n",
        "       'cor_interior', 'ee_cabine', 'ee_novo', 'comprimento', 'data_listagem',\n",
        "       'cor_listagem', 'nome_marca', 'maximo_assentos', 'quilometragem',\n",
        "       'nome_modelo', 'qtd_proprietarios', 'potencia', 'preco', 'recuperado',\n",
        "       'valor_economizado', 'avaliacao_vendedor', 'nome_vendedor',\n",
        "       'titulo_roubo', 'torque', 'transmissao', 'exibicao_transmissao',\n",
        "       'nome_versao', 'sistema_rodas', 'exibicao_sistema_rodas', 'entre_eixos',\n",
        "       'largura', 'ano']\n",
        "  )\n",
        "  important = importances.to_frame()\n",
        "  important.reset_index(inplace=True)\n",
        "  important.columns = ['Importância','Feature', ]\n",
        "  st.markdown('##### Gráfico de Importância de parametros')\n",
        "  px.bar(data_frame=important, x='Feature', y='Importância', orientation='h', template='plotly_dark')\n",
        "  table_report(y_test, prevision_random_forest, 'Random Forest')\n",
        "  confusion_graph(y_test, prevision_random_forest, 'Random Forest')\n",
        "\n",
        "\n",
        "def KNN(\n",
        "  x_training: np.ndarray, y_training: np.ndarray, x_test: np.ndarray, y_test: np.ndarray\n",
        "  )-> None:\n",
        "  st.markdown('### Resultado do machine learning usando o método KNN')\n",
        "  if not(os.path.isfile('KNN_data.pkl')):\n",
        "    obj_knn = KNeighborsClassifier(n_neighbors=10, weights='distance', p=1)\n",
        "    obj_knn.fit(x_training, y_training)\n",
        "    with open('KNN_data.pkl', mode='wb') as f:\n",
        "      pickle.dump(obj_knn, f)\n",
        "  else:\n",
        "    with open('KNN_data.pkl', 'rb') as f:\n",
        "      obj_knn = pickle.load(f)\n",
        "  prevision_knn = obj_knn.predict(x_test)\n",
        "  table_report(y_test, prevision_knn, 'KNN')\n",
        "  confusion_graph(y_test, prevision_knn, 'KNN')\n",
        "\n",
        "\n",
        "if not(os.path.isfile('data/usedcars_usa.pkl')):\n",
        "  print('iniciando...')\n",
        "  main()\n",
        "\n",
        "with open('data/usedcars_usa.pkl', 'rb') as f:\n",
        "    X_training, X_test, y_training, y_test = pickle.load(f)\n",
        "\n",
        "\n",
        "tree_decision(X_training, y_training, X_test, y_test)\n",
        "random_forest(X_training, y_training, X_test, y_test)\n",
        "KNN(X_training, y_training, X_test, y_test)\n",
        "\n",
        "def confusion_graph(y_test, previsao, method:str = ''):\n",
        "  #st.markdown(f'##### Matriz de Confução do metodo: <span style=\"color: blue\">{method}</span>', unsafe_allow_html=True)\n",
        "  labels = sorted(list(set(y_test) | set(previsao)))\n",
        "  cm = pd.DataFrame(0, index=labels, columns=labels)\n",
        "  for true_label, predicted_label in zip(y_test, previsao):\n",
        "      cm.loc[true_label, predicted_label] += 1\n",
        "  #st.table(cm)\n",
        "  print(cm)\n",
        "\n",
        "\n",
        "def tree_decision(\n",
        "  x_training: np.ndarray, y_training: np.ndarray, x_test: np.ndarray, y_test: np.ndarray\n",
        "  )-> None:\n",
        "  #st.markdown('### Resultado do machine learning usando o método Árvore de decisão')\n",
        "  if not(os.path.isfile('tree_decision.pkl')):\n",
        "    obj_tree_decision = DecisionTreeClassifier(criterion='entropy')\n",
        "    obj_tree_decision.fit(X_training, y_training)\n",
        "    with open('tree_decision.pkl', mode='wb') as f:\n",
        "      pickle.dump(obj_tree_decision, f)\n",
        "  else:\n",
        "    with open('tree_decision.pkl', 'rb') as f:\n",
        "      obj_tree_decision = pickle.load(f)\n",
        "  prevision_tree = obj_tree_decision.predict(x_test)\n",
        "  table_report(y_test, prevision_tree,'Árvore de decisão')\n",
        "  confusion_graph(y_test, prevision_tree, 'Árvore de decisão')"
      ],
      "metadata": {
        "id": "P1eexWLMqj_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressao"
      ],
      "metadata": {
        "id": "FI3jBtOBn0op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Avalia múltiplos modelos de regressão e retorna uma tabela com as métricas de desempenho.\n",
        "\n",
        "    :param models: Dicionário de modelos para avaliar { 'nome_modelo': modelo }\n",
        "    :param X_train: Conjunto de treino (features)\n",
        "    :param y_train: Conjunto de treino (target)\n",
        "    :param X_test: Conjunto de teste (features)\n",
        "    :param y_test: Conjunto de teste (target)\n",
        "    :return: DataFrame com as métricas de avaliação (MSE, RMSE, MAE, R²)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        results.append({\n",
        "            'Modelo': model_name,\n",
        "            'MSE': mse,\n",
        "            'RMSE': rmse,\n",
        "            'MAE': mae,\n",
        "            'R²': r2\n",
        "        })\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df\n",
        "models = {\n",
        "    'Ridge Regression': Ridge(alpha=1.0),\n",
        "    'Support Vector Machine (RBF Kernel)': SVR(kernel='rbf'),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100),\n",
        "    'XGBoost': XGBRegressor(n_estimators=100),\n",
        "    'SVM Linear': SVR(kernel='linear')\n",
        "}\n",
        "\n",
        "results_df = evaluate_models(models, X_training_regressor, y_training_regressor, X_test_regressor, y_test_regressor)\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "0fLJo0XiqlN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clusterizacao"
      ],
      "metadata": {
        "id": "otTB0_-anTrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "X_clusters = df.iloc[:,[4,28,32]].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_clusters = scaler.fit_transform(X_clusters)\n",
        "\n",
        "\n",
        "\n",
        "wcss = []\n",
        "\n",
        "for i in range(1,11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "    kmeans.fit(X_clusters)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "\n",
        "\n",
        "graph_wcss = px.line(x = range(1,11), y=wcss)\n",
        "graph_wcss\n",
        "\n",
        "\n",
        "\n",
        "kmeans_usedcars = KMeans(n_clusters=4, random_state=0)\n",
        "labels = kmeans_usedcars.fit_predict(X_clusters)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_clusters_pca = pca.fit_transform(X_clusters)\n",
        "\n",
        "\n",
        "graph_clusters = px.scatter(x=X_clusters_pca[:,0], y= X_clusters_pca[:,1], color=labels)\n",
        "graph_clusters.show()\n",
        "\n",
        "colunas_cluster = ['preco', 'quilometragem', 'cavalo_de_potencia', 'consumo_cidade', 'dias_no_mercado']\n",
        "df_cluster = df[colunas_cluster]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df_cluster)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Ajuste eps e min_samples conforme necessário\n",
        "clusters_dbscan = dbscan.fit_predict(df_scaled)\n",
        "\n",
        "# Adicionar os clusters ao dataset original\n",
        "df['cluster_dbscan'] = clusters_dbscan\n",
        "\n",
        "# Contar o número de clusters gerados (-1 é o ruído)\n",
        "num_clusters = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)\n",
        "print(f\"Número de clusters encontrados: {num_clusters}\")\n",
        "\n",
        "# Avaliar a qualidade dos clusters (somente se houver mais de um cluster)\n",
        "if num_clusters > 1:\n",
        "    silhouette_avg_dbscan = silhouette_score(df_scaled, clusters_dbscan)\n",
        "    print(f\"Índice de Silhueta para DBSCAN: {silhouette_avg_dbscan}\")\n",
        "\n",
        "# Visualização dos clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['preco'], df['quilometragem'], c=df['cluster_dbscan'], cmap='viridis', marker='o', s=50)\n",
        "plt.xlabel('Preço')\n",
        "plt.ylabel('Quilometragem')\n",
        "plt.title('Clusters usando DBSCAN')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "czViTMNhqmS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KMeans"
      ],
      "metadata": {
        "id": "OfWafN_3pNs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_clusters = df.iloc[:,[4,28,32]].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_clusters = scaler.fit_transform(X_clusters)\n",
        "\n",
        "\n",
        "\n",
        "wcss = []\n",
        "\n",
        "for i in range(1,11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "    kmeans.fit(X_clusters)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "\n",
        "\n",
        "graph_wcss = px.line(x = range(1,11), y=wcss)\n",
        "graph_wcss\n",
        "\n",
        "\n",
        "\n",
        "kmeans_usedcars = KMeans(n_clusters=4, random_state=0)\n",
        "labels = kmeans_usedcars.fit_predict(X_clusters)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_clusters_pca = pca.fit_transform(X_clusters)\n",
        "\n",
        "\n",
        "graph_clusters = px.scatter(x=X_clusters_pca[:,0], y= X_clusters_pca[:,1], color=labels)\n",
        "graph_clusters.show()\n"
      ],
      "metadata": {
        "id": "EqlxxC10pcwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DBScan"
      ],
      "metadata": {
        "id": "oBl8zYMvpQoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "colunas_cluster = ['preco', 'quilometragem', 'cavalo_de_potencia', 'consumo_cidade', 'dias_no_mercado']\n",
        "df_cluster = df[colunas_cluster]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df_cluster)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Ajuste eps e min_samples conforme necessário\n",
        "clusters_dbscan = dbscan.fit_predict(df_scaled)\n",
        "\n",
        "# Adicionar os clusters ao dataset original\n",
        "df['cluster_dbscan'] = clusters_dbscan\n",
        "\n",
        "# Contar o número de clusters gerados (-1 é o ruído)\n",
        "num_clusters = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)\n",
        "print(f\"Número de clusters encontrados: {num_clusters}\")\n",
        "\n",
        "# Avaliar a qualidade dos clusters (somente se houver mais de um cluster)\n",
        "if num_clusters > 1:\n",
        "    silhouette_avg_dbscan = silhouette_score(df_scaled, clusters_dbscan)\n",
        "    print(f\"Índice de Silhueta para DBSCAN: {silhouette_avg_dbscan}\")\n",
        "\n",
        "# Visualização dos clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['preco'], df['quilometragem'], c=df['cluster_dbscan'], cmap='viridis', marker='o', s=50)\n",
        "plt.xlabel('Preço')\n",
        "plt.ylabel('Quilometragem')\n",
        "plt.title('Clusters usando DBSCAN')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MdymfXf_aSMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WJIfOyrqbFMJ"
      }
    }
  ]
}